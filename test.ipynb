{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halcyon/anaconda3/envs/dukeai/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to parse 15 pdfs...\n",
      "src/2021-carbon-free-energy-data-centers.pdf\n",
      "src/2024-data-centre-industry-outlook.pdf\n",
      "src/ADL_Green_data_centers_2023.pdf\n",
      "src/bnef-eaton-statkraft-data-center-study-en-us.pdf\n",
      "src/CalData_Californias_Data_Strategy_2020.pdf\n",
      "src/CEC-400-2022-010_CMF.pdf\n",
      "src/CyrusOne-2024-Sustainability-Report.pdf\n",
      "src/datacenters_roadmap_final_0.pdf\n",
      "src/Datacenter_Sustainability_Strategy_Brief.pdf\n",
      "src/decarbonizing-the-data-center-industry-siemens-2021.pdf\n",
      "src/Equinix-Inc_2023-Sustainability-Report-3.pdf\n",
      "src/jll-us-data-center-report-h1-2024.pdf\n",
      "src/North America Data Center Trends H1 2024.pdf\n",
      "src/Report_Digital_Realty_2406_2023_ESG_Report.pdf\n",
      "src/UI Field report 152_Annual survey - Supply view.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Ignoring wrong pointing object 125 0 (offset 0)\n",
      "Ignoring wrong pointing object 127 0 (offset 0)\n",
      "Ignoring wrong pointing object 206 0 (offset 0)\n",
      "Ignoring wrong pointing object 275 0 (offset 0)\n",
      "Ignoring wrong pointing object 297 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished parsing, starting to add to vector store...\n"
     ]
    }
   ],
   "source": [
    "async def parse_pdf(path):\n",
    "    print(path)\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = []\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True)\n",
    "    page_splits = text_splitter.split_documents(pages)\n",
    "    return page_splits\n",
    "\n",
    "\n",
    "async def parse_txt(path):\n",
    "    print(path)\n",
    "    with open(path, \"r\", errors=\"ignore\") as file:\n",
    "        text = file.read()\n",
    "    with open(f\"{path}\", \"w\") as file:\n",
    "        file.write(text)\n",
    "    loader = CSVLoader(path)\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, add_start_index=True)\n",
    "    page_splits = text_splitter.split_documents(data)\n",
    "    return page_splits\n",
    "\n",
    "\n",
    "async def parse_external_data():\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"data_center_collection\",\n",
    "        embedding_function=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),\n",
    "    )\n",
    "    file_dir = \"data_src\"\n",
    "    docs = []\n",
    "    pdf_paths = [os.path.join(file_dir, path) for path in os.listdir(file_dir) if path.endswith(\".pdf\")]\n",
    "    txt_paths = [os.path.join(file_dir, path) for path in os.listdir(file_dir) if path.endswith(\".txt\")]\n",
    "    # print(f\"starting to parse {len(txt_paths)} txts...\")\n",
    "    # parsed_docs = await asyncio.gather(*(parse_txt(path) for path in txt_paths))\n",
    "    # docs = [page for pages in parsed_docs for page in pages]\n",
    "    # print(\"finished parsing, starting to add to vector store...\")\n",
    "    # vector_store.add_documents(docs)\n",
    "\n",
    "    print(f\"starting to parse {len(pdf_paths)} pdfs...\")\n",
    "    parsed_docs = await asyncio.gather(*(parse_pdf(path) for path in pdf_paths))\n",
    "    docs = [page for pages in parsed_docs for page in pages]\n",
    "    print(\"finished parsing, starting to add to vector store...\")\n",
    "    vector_store.add_documents(docs)\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "vector_store = await parse_external_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval and Generation: Retrieve\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# Retrieval and Generation: Generation\n",
    "llm = ChatGroq(api_key=os.getenv(\"GROQ_API_KEY\"), model=\"llama-3.2-1b-preview\")\n",
    "\n",
    "# prompt engeering for RAG model\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "system_prompt = \"\"\"\n",
    "You are an consultant for question-answering tasks for building datacenters. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "Specify the resource you used in your answer. \n",
    "If you don't know the answer, just say that you don't know.\n",
    "Consider different aspects OF building datacenters, including but not limited to: scope1,2,3 emissions, water and land usage.\n",
    "For each question, you can provide multiple aspects related to the original question so the user can dive deeper into the topic.\n",
    "List out specific companies, technologies, or strategies that are mentioned in the retrieved  context.\n",
    "Input: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "system_prompt = (\n",
    "    \"You are an consultant for question-answering tasks for building datacenters. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"Specify the resource you used in your answer. \"\n",
    "    \"the question. If you don't know the answer, say that you don't know.\"\n",
    "    \"Consider different aspects OF building datacenters, including but not limited to: scope 1,2,3 emissions, water and land usage.\"\n",
    "    \"For each question, you can provide multiple aspects related to the original question so the user can dive deeper into the topic.\"\n",
    "    \"List out specific companies, technologies, or strategies that are mentioned in the retrieved context.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# We define a dict representing the state of the application.\n",
    "# This state has the same input and output keys as `rag_chain`.\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# We then define a simple node that runs the `rag_chain`.\n",
    "# The `return` values of the node update the graph state, so here we just\n",
    "# update the chat history with the input message and response.\n",
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Our graph consists only of one node:\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Finally, we compile the graph with a checkpointer object.\n",
    "# This persists the state, in this case in memory.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fastapi import FastAPI\n",
    "\n",
    "fastapi = FastAPI()\n",
    "\n",
    "@fastapi.get(\"/ask\")\n",
    "def ask_meaning():\n",
    "    config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "    result = app.invoke(\n",
    "        {\"input\": \"What is Task Decomposition?\"},\n",
    "        config=config,\n",
    "    )\n",
    "    return {\"response\": result['answer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [235762]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:60522 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:60532 - \"GET /ask HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [235762]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import uvicorn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(fastapi)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a method of breaking down complex tasks into smaller, more manageable tasks that can be performed individually or in groups. It involves identifying the specific activities, steps, and processes required to complete a task or a project, and then dividing it into smaller tasks that can be executed separately.\n",
      "\n",
      "Task decomposition is often used in project management, IT, and other fields to:\n",
      "\n",
      "1. Break down complex tasks into smaller, more manageable ones\n",
      "2. Reduce complexity and make tasks more understandable\n",
      "3. Improve communication and collaboration among team members\n",
      "4. Enhance efficiency and productivity\n",
      "5. Reduce risk and errors\n",
      "\n",
      "Task decomposition involves several steps:\n",
      "\n",
      "1. Identify the task or project\n",
      "2. Map out the task steps and activities\n",
      "3. Identify the inputs, outputs, and deliverables\n",
      "4. Determine the required resources and skills\n",
      "5. Prioritize the tasks and create a project schedule\n",
      "\n",
      "Task decomposition can be used for various purposes, such as:\n",
      "\n",
      "1. Defining project scope and requirements\n",
      "2. Creating a project plan and schedule\n",
      "3. Assigning tasks to team members\n",
      "4. Identifying dependencies and risks\n",
      "5. Evaluating the project's progress and status\n",
      "\n",
      "In the context of the provided text, task decomposition is mentioned in the context of building datacenters, where it is used to:\n",
      "\n",
      "1. Break down the \"Day\" process into individual tasks such as succession planning, professional development, and community impact assessments.\n",
      "2. Decompose the \"growth and enhanced resilience\" objective into smaller tasks such as deploying guidance, developing testing protocols, and monitoring projections.\n",
      "3. Break down the construction project into smaller tasks such as developing design and engineering strategies, legal and compliance, and procurement and supply chain management.\n",
      "\n",
      "By using task decomposition, teams can better understand the requirements and responsibilities of each task, reduce complexity, and improve productivity.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "result = app.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One way to do task decomposition is the \"Starfish Decomposition Method\" or \"Fishbone Diagram\" (also known as Ishikawa Diagram). This method involves identifying the roots of a problem and decomposing them into smaller, separate areas of focus.\n",
      "\n",
      "The method consists of a simple diagram with a root cause (the main problem or issue) at the center, surrounded by several branches or lines that represent different possible causes. Each branch or line represents a specific aspect of the problem or issue, and the tasks associated with each branch or line are the steps needed to resolve it.\n",
      "\n",
      "Here's an example of how the Starfish Decomposition Method could be applied:\n",
      "\n",
      "* **Root Cause**: Construction project delays\n",
      "* **Branches**: Branch 1: Overly complex design\n",
      "\t+ Task 1: Review design plans\n",
      "\t+ Task 2: Check for regulatory compliance\n",
      "\t+ Task 3: Develop design revisions\n",
      "* Branch 2: Supply chain issues\n",
      "\t+ Task 4: Negotiate with suppliers\n",
      "\t+ Task 5: Manage inventory levels\n",
      "* Branch 3: Budget constraints\n",
      "\t+ Task 6: Review budget projections\n",
      "\t+ Task 7: Manage cost overruns\n",
      "* ...\n",
      "\n",
      "By applying the Starfish Decomposition Method, you can identify the different aspects of the problem or issue, and break them down into smaller, manageable tasks that can be addressed individually or in groups.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"What is one way of doing it?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dukeai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
